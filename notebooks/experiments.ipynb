{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classifiers.knn import Knn\n",
    "from src.evaluation.evaluation import Evaluator\n",
    "from src.preprocessing.preprocessing import Preprocessor\n",
    "from src.data_representations.bow import BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== NOTE: This notebook is deprecated ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set BOW approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and initializing KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "filepath_train = \"./data/songs_train.txt\"\n",
    "dataset_train = Preprocessor(filepath=filepath_train, read_limit=10000)\n",
    "filepath_test = \"./data/songs_test.txt\"\n",
    "dataset_test = Preprocessor(filepath=filepath_test, read_limit=100)\n",
    "# Create numerical representations of labels for mapping\n",
    "label_to_num = {artist:i for i, artist in enumerate(set(dataset_train.artists) | set(dataset_test.artists))}\n",
    "num_to_label = {value:key for key, value in label_to_num.items()}\n",
    "# Initiate Knn classifier\n",
    "training_examples = [BOW(tok) for tok in dataset_train.tokenized]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "classifier = Knn(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.07\n",
      "Micro Precision:\n",
      " 0.3181818181818182\n",
      "Micro Recall:\n",
      " 0.07\n",
      "Micro F-Score:\n",
      " 0.11475409836065574\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "test_examples = dataset_test.BOW()\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "predictions = classifier.predict(test_examples, k=4, measure=\"jaccard\")\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SÃ¸rensen-Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.07\n",
      "Micro Precision:\n",
      " 0.3181818181818182\n",
      "Micro Recall:\n",
      " 0.07\n",
      "Micro F-Score:\n",
      " 0.11475409836065574\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"dsc\")\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Overlap index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.047619047619047616\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.01652892561983471\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"overlap\")\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using different Tversky settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.1$, $\\beta=0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.03\n",
      "Micro Precision:\n",
      " 0.15789473684210525\n",
      "Micro Recall:\n",
      " 0.03\n",
      "Micro F-Score:\n",
      " 0.050420168067226885\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.1, beta=0.9)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.2$, $\\beta=0.8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.3125\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08620689655172414\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.2, beta=0.8)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.3$, $\\beta=0.7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.3125\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08620689655172414\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.3, beta=0.7)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=\\frac{1}{3}$, $\\beta=\\frac{2}{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.2777777777777778\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08474576271186442\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=1/3, beta=2/3)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.4$, $\\beta=0.6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.22727272727272727\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08196721311475409\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.4, beta=0.6)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.6$, $\\beta=0.4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.20833333333333334\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08064516129032258\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.6, beta=0.4)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=\\frac{2}{3}$, $\\beta=\\frac{1}{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.20833333333333334\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08064516129032258\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=2/3, beta=1/3)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.7$, $\\beta=0.3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.06\n",
      "Micro Precision:\n",
      " 0.2222222222222222\n",
      "Micro Recall:\n",
      " 0.06\n",
      "Micro F-Score:\n",
      " 0.09448818897637795\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.7, beta=0.3)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.8$, $\\beta=0.2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.06\n",
      "Micro Precision:\n",
      " 0.18181818181818182\n",
      "Micro Recall:\n",
      " 0.06\n",
      "Micro F-Score:\n",
      " 0.09022556390977443\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.8, beta=0.2)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.1$, $\\beta=0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.03125\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.015151515151515152\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.9, beta=0.1)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=1$, $\\beta=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.03225806451612903\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.015267175572519083\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=1, beta=0)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0$, $\\beta=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.02\n",
      "Micro Precision:\n",
      " 0.11764705882352941\n",
      "Micro Recall:\n",
      " 0.02\n",
      "Micro F-Score:\n",
      " 0.03418803418803419\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0, beta=1)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running fairly good performing distance measures on full train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SÃ¸rensen-Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05013009540329575\n",
      "Micro Precision:\n",
      " 0.05029585798816568\n",
      "Micro Recall:\n",
      " 0.05013009540329575\n",
      "Micro F-Score:\n",
      " 0.05021283989227695\n"
     ]
    }
   ],
   "source": [
    "# Read dataset\n",
    "filepath_train = \"./data/songs_train.txt\"\n",
    "dataset_train = Preprocessor(filepath=filepath_train, read_limit=46120)\n",
    "filepath_test = \"./data/songs_test.txt\"\n",
    "dataset_test = Preprocessor(filepath=filepath_test, read_limit=5765)\n",
    "# Create numerical representations of labels for mapping\n",
    "label_to_num = {artist:i for i, artist in enumerate(set(dataset_train.artists) | set(dataset_test.artists))}\n",
    "num_to_label = {value:key for key, value in label_to_num.items()}\n",
    "# Initiate Knn classifier\n",
    "training_examples = [BOW(tok) for tok in dataset_train.tokenized]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "classifier = Knn(training_examples, training_labels)\n",
    "# Run evaluation of algorithms performance\n",
    "test_examples = [BOW(tok) for tok in dataset_test.tokenized]\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "predictions = classifier.predict(test_examples, k=4, measure=\"dsc\")\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "filepath_train = \"./data/songs_train.txt\"\n",
    "dataset_train = Preprocessor(filepath=filepath_train, read_limit=46120)\n",
    "filepath_test = \"./data/songs_test.txt\"\n",
    "dataset_test = Preprocessor(filepath=filepath_test, read_limit=5765)\n",
    "# Create numerical representations of labels for mapping\n",
    "label_to_num = {artist:i for i, artist in enumerate(set(dataset_train.artists) | set(dataset_test.artists))}\n",
    "num_to_label = {value:key for key, value in label_to_num.items()}\n",
    "# Initiate Knn classifier\n",
    "training_examples = [BOW(tok) for tok in dataset_train.tokenized]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "classifier = Knn(training_examples, training_labels)\n",
    "# Run evaluation of algorithms performance\n",
    "test_examples = [BOW(tok) for tok in dataset_test.tokenized]\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05013009540329575\n",
      "Micro Precision:\n",
      " 0.05029585798816568\n",
      "Micro Recall:\n",
      " 0.05013009540329575\n",
      "Micro F-Score:\n",
      " 0.05021283989227695\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"jaccard\")\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.049262792714657416\n",
      "Micro Precision:\n",
      " 0.04941708717591787\n",
      "Micro Recall:\n",
      " 0.049262792714657416\n",
      "Micro F-Score:\n",
      " 0.04933981931897151\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=3, measure=\"jaccard\")\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.04700780572419774\n",
      "Micro Precision:\n",
      " 0.04715503741082304\n",
      "Micro Recall:\n",
      " 0.04700780572419774\n",
      "Micro F-Score:\n",
      " 0.0470813064628214\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=2, measure=\"jaccard\")\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.04700780572419774\n",
      "Micro Precision:\n",
      " 0.04715503741082304\n",
      "Micro Recall:\n",
      " 0.04700780572419774\n",
      "Micro F-Score:\n",
      " 0.0470813064628214\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=1, measure=\"jaccard\")\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05151777970511708\n",
      "Micro Precision:\n",
      " 0.051688130873651233\n",
      "Micro Recall:\n",
      " 0.05151777970511708\n",
      "Micro F-Score:\n",
      " 0.051602814698983576\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=5, measure=\"jaccard\")\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_representations.tf_idf import TfIdf\n",
    "from src.data_representations.vector import Vector\n",
    "from src.classifiers.knn import Knn\n",
    "from src.preprocessing.preprocessing import Preprocessor\n",
    "from src.evaluation.evaluation import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "filepath_train = \"./data/songs_train.txt\"\n",
    "dataset_train = Preprocessor(filepath=filepath_train, read_limit=1000)\n",
    "tf_idf = TfIdf()\n",
    "train = tf_idf.fit_transform(dataset_train.tokenized)\n",
    "filepath_test = \"./data/songs_test.txt\"\n",
    "dataset_test = Preprocessor(filepath=filepath_test, read_limit=100)\n",
    "test = tf_idf.transform(dataset_test.tokenized)\n",
    "# Create numerical representations of labels for mapping\n",
    "label_to_num = {artist:i for i, artist in enumerate(set(dataset_train.artists) | set(dataset_test.artists))}\n",
    "num_to_label = {value:key for key, value in label_to_num.items()}\n",
    "# Initiate Knn classifier\n",
    "training_examples = [Vector([ex]) for ex in train]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "classifier = Knn(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 316, 297, 20, 50, 11, 201, 20, 20, 20, 213, 138, 201, 264, 201, 342, 394, 316, 350, 20, 252, 136, 201, 20, 213, 243, 20, 20, 136, 20, 316, 408, 336, 20, 201, 20, 213, 20, 213, 20, 20, 264, 20, 213, 20, 20, 108, 20, 20, 20, 264, 20, 201, 306, 20, 160, 316, 20, 215, 333, 20, 20, 20, 20, 108, 135, 264, 333, 20, 304, 136, 20, 316, 342, 297, 213, 20, 63, 297, 20, 342, 20, 174, 215, 342, 201, 174, 213, 20, 213, 20, 342, 394, 201, 20, 213, 264, 213, 342, 215]\n"
     ]
    }
   ],
   "source": [
    "test_examples = [Vector([ex]) for ex in test]\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "predictions = classifier.predict(test_examples, k=4, measure=\"euclidean\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kyla  -  Virgin Steele\n",
      "Lil Wayne  -  Demi Lovato\n",
      "Counting Crows  -  Indigo Girls\n",
      "Kyla  -  Phil Collins\n",
      "Lionel Richie  -  Extreme\n",
      "The Temptations  -  Vince Gill\n",
      "Bosson  -  Dusty Springfield\n",
      "Kyla  -  Nat King Cole\n",
      "Kyla  -  Arrogant Worms\n",
      "Kyla  -  Kirsty Maccoll\n",
      "LL Cool J  -  Chuck Berry\n",
      "Avril Lavigne  -  Avril Lavigne\n",
      "Bosson  -  Van Halen\n",
      "Backstreet Boys  -  Phil Collins\n",
      "Bosson  -  Rod Stewart\n",
      "U2  -  Mark Ronson\n",
      "Tim McGraw  -  Michael Bolton\n",
      "Lil Wayne  -  Gipsy Kings\n",
      "The Beatles  -  Linda Ronstadt\n",
      "Kyla  -  Rihanna\n",
      "Stone Temple Pilots  -  Queen\n",
      "Outkast  -  Kanye West\n",
      "Bosson  -  ABBA\n",
      "Kyla  -  Religious Music\n",
      "LL Cool J  -  Queen Latifah\n",
      "Ice Cube  -  Snoop Dogg\n",
      "Kyla  -  Kid Rock\n",
      "Kyla  -  Michael Bolton\n",
      "Outkast  -  Lil Wayne\n",
      "Kyla  -  Alison Krauss\n",
      "Lil Wayne  -  Michael Bolton\n",
      "Bob Dylan  -  Veruca Salt\n",
      "Everlast  -  Bryan White\n",
      "Kyla  -  Cher\n",
      "Bosson  -  George Jones\n",
      "Kyla  -  Lauryn Hill\n",
      "LL Cool J  -  Green Day\n",
      "Kyla  -  Quietdrive\n",
      "LL Cool J  -  Kid Rock\n",
      "Kyla  -  John Prine\n",
      "Kyla  -  Kinks\n",
      "Backstreet Boys  -  Judas Priest\n",
      "Kyla  -  Iggy Pop\n",
      "LL Cool J  -  Iron Maiden\n",
      "Kyla  -  Uriah Heep\n",
      "Kyla  -  Frank Sinatra\n",
      "Regine Velasquez  -  Christmas Songs\n",
      "Kyla  -  Scorpions\n",
      "Kyla  -  Helloween\n",
      "Kyla  -  Moody Blues\n",
      "Backstreet Boys  -  Otis Redding\n",
      "Kyla  -  Slayer\n",
      "Bosson  -  Radiohead\n",
      "Zebrahead  -  Xscape\n",
      "Kyla  -  Red Hot Chili Peppers\n",
      "Everclear  -  Grateful Dead\n",
      "Lil Wayne  -  Incubus\n",
      "Kyla  -  Kinks\n",
      "Out Of Eden  -  Evanescence\n",
      "Luther Vandross  -  Yoko Ono\n",
      "Kyla  -  Eurythmics\n",
      "Kyla  -  Roy Orbison\n",
      "Kyla  -  Fiona Apple\n",
      "Kyla  -  Conway Twitty\n",
      "Regine Velasquez  -  ABBA\n",
      "George Michael  -  U. D. O.\n",
      "Backstreet Boys  -  Air Supply\n",
      "Luther Vandross  -  Faith No More\n",
      "Kyla  -  Bread\n",
      "Kirsty Maccoll  -  Paul McCartney\n",
      "Outkast  -  Nina Simone\n",
      "Kyla  -  Steely Dan\n",
      "Lil Wayne  -  ZZ Top\n",
      "U2  -  Yes\n",
      "Counting Crows  -  David Allan Coe\n",
      "LL Cool J  -  J Cole\n",
      "Kyla  -  Rod Stewart\n",
      "Neil Sedaka  -  Kenny Rogers\n",
      "Counting Crows  -  Guided By Voices\n",
      "Kyla  -  Marilyn Manson\n",
      "U2  -  Status Quo\n",
      "Kyla  -  Dean Martin\n",
      "Hank Williams Jr.  -  The Beatles\n",
      "Out Of Eden  -  Maroon 5\n",
      "U2  -  Within Temptation\n",
      "Bosson  -  Emmylou Harris\n",
      "Hank Williams Jr.  -  'n Sync\n",
      "LL Cool J  -  Def Leppard\n",
      "Kyla  -  Christina Perri\n",
      "LL Cool J  -  Everlast\n",
      "Kyla  -  John Prine\n",
      "U2  -  Queens Of The Stone Age\n",
      "Tim McGraw  -  Nat King Cole\n",
      "Bosson  -  Lucky Dube\n",
      "Kyla  -  Fall Out Boy\n",
      "LL Cool J  -  Venom\n",
      "Backstreet Boys  -  Prince\n",
      "LL Cool J  -  Xiu Xiu\n",
      "U2  -  Britney Spears\n",
      "Out Of Eden  -  Nickelback\n"
     ]
    }
   ],
   "source": [
    "for pred, lab in zip(predictions, test_labels):\n",
    "    print(num_to_label[pred], \" - \", num_to_label[lab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.1111111111111111\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.01834862385321101\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[330, 169, 297, 398, 157, 11, 428, 173, 118, 425, 440, 162, 92, 264, 289, 241, 352, 213, 350, 52, 252, 197, 197, 403, 213, 303, 414, 292, 303, 402, 65, 175, 164, 301, 28, 400, 175, 240, 86, 361, 6, 301, 106, 109, 45, 220, 108, 326, 169, 20, 196, 128, 329, 306, 81, 8, 169, 399, 84, 333, 224, 265, 99, 214, 197, 405, 264, 333, 196, 304, 359, 421, 385, 207, 312, 174, 148, 63, 65, 403, 250, 24, 178, 357, 301, 328, 174, 257, 9, 104, 26, 93, 8, 270, 343, 354, 264, 337, 23, 250]\n"
     ]
    }
   ],
   "source": [
    "test_examples = [Vector([ex]) for ex in test]\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "predictions = classifier.predict(test_examples, k=4, measure=\"cosine\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.02\n",
      "Micro Precision:\n",
      " 0.125\n",
      "Micro Recall:\n",
      " 0.02\n",
      "Micro F-Score:\n",
      " 0.03448275862068966\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test size as with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "filepath_train = \"./data/songs_train.txt\"\n",
    "dataset_train = Preprocessor(filepath=filepath_train, read_limit=10000)\n",
    "tf_idf = TfIdf()\n",
    "train = tf_idf.fit_transform(dataset_train.tokenized)\n",
    "filepath_test = \"./data/songs_test.txt\"\n",
    "dataset_test = Preprocessor(filepath=filepath_test, read_limit=100)\n",
    "test = tf_idf.transform(dataset_test.tokenized)\n",
    "# Create numerical representations of labels for mapping\n",
    "label_to_num = {artist:i for i, artist in enumerate(set(dataset_train.artists) | set(dataset_test.artists))}\n",
    "num_to_label = {value:key for key, value in label_to_num.items()}\n",
    "# Initiate Knn classifier\n",
    "training_examples = [Vector([ex]) for ex in train]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "classifier = Knn(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [Vector([ex]) for ex in test]\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "predictions = classifier.predict(test_examples, k=4, measure=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.07\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision:\n",
      " 0.23333333333333334\n",
      "Micro Recall:\n",
      " 0.07\n",
      "Micro F-Score:\n",
      " 0.1076923076923077\n"
     ]
    }
   ],
   "source": [
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = classifier.predict(test_examples, k=4, measure=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(test_labels, predictions2)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision:\n",
      " 0.09090909090909091\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.018018018018018018\n"
     ]
    }
   ],
   "source": [
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "filepath_train = \"./data/songs_train.txt\"\n",
    "dataset_train = Preprocessor(filepath=filepath_train, read_limit=46120)\n",
    "tf_idf = TfIdf()\n",
    "train = tf_idf.fit_transform(dataset_train.tokenized)\n",
    "filepath_test = \"./data/songs_test.txt\"\n",
    "dataset_test = Preprocessor(filepath=filepath_test, read_limit=5765)\n",
    "test = tf_idf.transform(dataset_test.tokenized)\n",
    "# Create numerical representations of labels for mapping\n",
    "label_to_num = {artist:i for i, artist in enumerate(set(dataset_train.artists) | set(dataset_test.artists))}\n",
    "num_to_label = {value:key for key, value in label_to_num.items()}\n",
    "# Initiate Knn classifier\n",
    "training_examples = [Vector([ex]) for ex in train]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "classifier = Knn(training_examples, training_labels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc866f6f9bb2552971eec93b1f215e09d6f69ca4316bcdf879e7f17b4fe78227"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
