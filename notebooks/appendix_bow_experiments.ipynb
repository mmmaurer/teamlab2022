{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classifiers.knn import Knn\n",
    "from src.evaluation.evaluation import Evaluator\n",
    "from src.preprocessing.preprocessing import Preprocessor\n",
    "from src.data_representations.bow import BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words experiments\n",
    "\n",
    "This notebook contains experiments for artist classification with k nearest neighbors with a set based bag of words approach as a representation for the lyrics (i.e. each lyric is represented by a set its unique words) in different configurations of different training and test sizes as well as distance metrics.\n",
    "\n",
    "The first part provides a tutorial on how to run the experiments and specify core hyperparameters and settings.\n",
    "\n",
    "The second part showcases experiments with their results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the experiments, either add your datasets to the data folder or change these variables to the paths to your datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_train = \"./data/songs_train.txt\"\n",
    "filepath_test = \"./data/songs_test.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run experiments with custom settings, change `read_limit` in the respective `Preprocessor`s to your training and test sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Preprocessor(filepath=filepath_train, read_limit=10000)\n",
    "dataset_test = Preprocessor(filepath=filepath_test, read_limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create numerical representations of labels for mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_num = {artist:i for i, artist in enumerate(set(dataset_train.artists) | set(dataset_test.artists))}\n",
    "num_to_label = {value:key for key, value in label_to_num.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and testing examples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = [BOW(tok) for tok in dataset_train.tokenized]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "test_examples = dataset_test.BOW()\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Knn(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use multiprocessing for the classifier, specify `multi_process`, the default is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Knn(training_examples, training_labels, multi_process=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the distance metric by changing the `measure` argument in `classifier.predict` from the following options\n",
    "\n",
    "- Overlap coefficient: `\"overlap\"`\n",
    "\n",
    "- Jaccard index: `\"jaccard\"`\n",
    "\n",
    "- Sørensen-Dice coefficient: `\"dsc\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "predictions = classifier.predict(test_examples, k=4, measure=\"jaccard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and Tversky: `\"tversky\"` with specifying $\\alpha$ and $\\beta$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=1, beta=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize evaluation with the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and evaluate with accuracy, micro and macro $F_1$, precision and recall as follows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.accuracy()\n",
    "evaluator.micro_precision()\n",
    "evaluator.micro_recall()\n",
    "evaluator.micro_f1()\n",
    "evaluator.macro_precision()\n",
    "evaluator.macro_recall()\n",
    "evaluator.macro_f1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filepaths\n",
    "filepath_train = \"./data/songs_train.txt\"\n",
    "filepath_test = \"./data/songs_test.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiments 1) and 2) are conducted with $k=4$, experiment 3) is conducted with $k=4$ for Sørensen-Dice and $k \\in \\{1,2,...,5\\}$.\n",
    "\n",
    "The distance metrics used here are:\n",
    "\n",
    "- Tversky index: $S(X,Y) = \\frac{|X \\cap Y|}{|X \\cap Y| + \\alpha |X \\setminus Y| + \\beta |Y \\setminus X|}$\n",
    "\n",
    "- Overlap coefficient: $overlap(X,Y) = \\frac{|X \\cap Y|}{min(|X|,|Y|)}$\n",
    "\n",
    "- Jaccard index: $J(A,B) = \\frac{|A\\cap B|}{|A \\cup B|} = \\frac{|A\\cap B|}{|A|+|B|-|A \\cap B|}$\n",
    "\n",
    "- Sørensen-Dice coefficient: $DSC(X,Y) = \\frac{2|X\\cap Y|}{|X|+|Y|}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datasets\n",
    "dataset_train = Preprocessor(filepath=filepath_train, read_limit=10000)\n",
    "dataset_test = Preprocessor(filepath=filepath_test, read_limit=100)\n",
    "# Create numerical representations of labels for mapping\n",
    "label_to_num = {artist:i for i, artist in enumerate(set(dataset_train.artists) | set(dataset_test.artists))}\n",
    "num_to_label = {value:key for key, value in label_to_num.items()}\n",
    "# Initiate Knn classifier\n",
    "training_examples = [BOW(tok) for tok in dataset_train.tokenized]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "classifier = Knn(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 10k training/100 test sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and initializing KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datasets\n",
    "dataset_train = Preprocessor(filepath=filepath_train, read_limit=10000)\n",
    "dataset_test = Preprocessor(filepath=filepath_test, read_limit=100)\n",
    "# Create numerical representations of labels for mapping\n",
    "label_to_num = {artist:i for i, artist in enumerate(set(dataset_train.artists) | set(dataset_test.artists))}\n",
    "num_to_label = {value:key for key, value in label_to_num.items()}\n",
    "# Initiate Knn classifier\n",
    "training_examples = [BOW(tok) for tok in dataset_train.tokenized]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "classifier = Knn(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.07\n",
      "Micro Precision:\n",
      " 0.3181818181818182\n",
      "Micro Recall:\n",
      " 0.07\n",
      "Micro F-Score:\n",
      " 0.11475409836065574\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "test_examples = dataset_test.BOW()\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "predictions = classifier.predict(test_examples, k=4, measure=\"jaccard\")\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sørensen-Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.07\n",
      "Micro Precision:\n",
      " 0.3181818181818182\n",
      "Micro Recall:\n",
      " 0.07\n",
      "Micro F-Score:\n",
      " 0.11475409836065574\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"dsc\")\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.047619047619047616\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.01652892561983471\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"overlap\")\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Tversky settings\n",
    "Experiments with different settings of $\\alpha$ and $\\beta$ for the Tversky index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.1$, $\\beta=0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.03\n",
      "Micro Precision:\n",
      " 0.15789473684210525\n",
      "Micro Recall:\n",
      " 0.03\n",
      "Micro F-Score:\n",
      " 0.050420168067226885\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.1, beta=0.9)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.2$, $\\beta=0.8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.3125\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08620689655172414\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.2, beta=0.8)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.3$, $\\beta=0.7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.3125\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08620689655172414\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.3, beta=0.7)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=\\frac{1}{3}$, $\\beta=\\frac{2}{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.2777777777777778\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08474576271186442\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=1/3, beta=2/3)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.4$, $\\beta=0.6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.22727272727272727\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08196721311475409\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.4, beta=0.6)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.6$, $\\beta=0.4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.20833333333333334\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08064516129032258\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.6, beta=0.4)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=\\frac{2}{3}$, $\\beta=\\frac{1}{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.20833333333333334\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08064516129032258\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=2/3, beta=1/3)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.7$, $\\beta=0.3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.06\n",
      "Micro Precision:\n",
      " 0.2222222222222222\n",
      "Micro Recall:\n",
      " 0.06\n",
      "Micro F-Score:\n",
      " 0.09448818897637795\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.7, beta=0.3)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.8$, $\\beta=0.2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.06\n",
      "Micro Precision:\n",
      " 0.18181818181818182\n",
      "Micro Recall:\n",
      " 0.06\n",
      "Micro F-Score:\n",
      " 0.09022556390977443\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.8, beta=0.2)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.1$, $\\beta=0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.03125\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.015151515151515152\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.9, beta=0.1)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=1$, $\\beta=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.03225806451612903\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.015267175572519083\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=1, beta=0)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0$, $\\beta=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.02\n",
      "Micro Precision:\n",
      " 0.11764705882352941\n",
      "Micro Recall:\n",
      " 0.02\n",
      "Micro F-Score:\n",
      " 0.03418803418803419\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0, beta=1)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 20k training/100 test size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and initializing KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datasets\n",
    "dataset_train = Preprocessor(filepath=filepath_train, read_limit=20000)\n",
    "dataset_test = Preprocessor(filepath=filepath_test, read_limit=100)\n",
    "# Create numerical representations of labels for mapping\n",
    "label_to_num = {artist:i for i, artist in enumerate(set(dataset_train.artists) | set(dataset_test.artists))}\n",
    "num_to_label = {value:key for key, value in label_to_num.items()}\n",
    "# Initiate Knn classifier\n",
    "training_examples = [BOW(tok) for tok in dataset_train.tokenized]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "classifier = Knn(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.09\n",
      "Micro Precision:\n",
      " 0.42857142857142855\n",
      "Micro Recall:\n",
      " 0.09\n",
      "Micro F-Score:\n",
      " 0.1487603305785124\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "test_examples = dataset_test.BOW()\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "predictions = classifier.predict(test_examples, k=4, measure=\"jaccard\")\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sørensen-Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.09\n",
      "Micro Precision:\n",
      " 0.42857142857142855\n",
      "Micro Recall:\n",
      " 0.09\n",
      "Micro F-Score:\n",
      " 0.1487603305785124\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "test_examples = dataset_test.BOW()\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "predictions = classifier.predict(test_examples, k=4, measure=\"dsc\")\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.05555555555555555\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.016949152542372885\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "test_examples = dataset_test.BOW()\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "predictions = classifier.predict(test_examples, k=4, measure=\"overlap\")\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tversky index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.25\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08333333333333334\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.7, beta=0.3)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Full training/test sizes (46,120/5,765)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and initializing KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datasets\n",
    "dataset_train = Preprocessor(filepath=filepath_train, read_limit=46120)\n",
    "dataset_test = Preprocessor(filepath=filepath_test, read_limit=5765)\n",
    "# Create numerical representations of labels for mapping\n",
    "label_to_num = {artist:i for i, artist in enumerate(set(dataset_train.artists) | set(dataset_test.artists))}\n",
    "num_to_label = {value:key for key, value in label_to_num.items()}\n",
    "# Initiate Knn classifier\n",
    "training_examples = [BOW(tok) for tok in dataset_train.tokenized]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "classifier = Knn(training_examples, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sørensen-Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05013009540329575\n",
      "Micro Precision:\n",
      " 0.05029585798816568\n",
      "Micro Recall:\n",
      " 0.05013009540329575\n",
      "Micro F-Score:\n",
      " 0.05021283989227695\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation of algorithms performance\n",
    "test_examples = [BOW(tok) for tok in dataset_test.tokenized]\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "predictions = classifier.predict(test_examples, k=4, measure=\"dsc\")\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard\n",
    "Experiments with $k \\in \\{1,2,...,5\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "filepath_train = \"./data/songs_train.txt\"\n",
    "dataset_train = Preprocessor(filepath=filepath_train, read_limit=46120)\n",
    "filepath_test = \"./data/songs_test.txt\"\n",
    "dataset_test = Preprocessor(filepath=filepath_test, read_limit=5765)\n",
    "# Create numerical representations of labels for mapping\n",
    "label_to_num = {artist:i for i, artist in enumerate(set(dataset_train.artists) | set(dataset_test.artists))}\n",
    "num_to_label = {value:key for key, value in label_to_num.items()}\n",
    "# Initiate Knn classifier\n",
    "training_examples = [BOW(tok) for tok in dataset_train.tokenized]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "classifier = Knn(training_examples, training_labels)\n",
    "# Run evaluation of algorithms performance\n",
    "test_examples = [BOW(tok) for tok in dataset_test.tokenized]\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05013009540329575\n",
      "Micro Precision:\n",
      " 0.05029585798816568\n",
      "Micro Recall:\n",
      " 0.05013009540329575\n",
      "Micro F-Score:\n",
      " 0.05021283989227695\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"jaccard\")\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.049262792714657416\n",
      "Micro Precision:\n",
      " 0.04941708717591787\n",
      "Micro Recall:\n",
      " 0.049262792714657416\n",
      "Micro F-Score:\n",
      " 0.04933981931897151\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=3, measure=\"jaccard\")\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.04700780572419774\n",
      "Micro Precision:\n",
      " 0.04715503741082304\n",
      "Micro Recall:\n",
      " 0.04700780572419774\n",
      "Micro F-Score:\n",
      " 0.0470813064628214\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=2, measure=\"jaccard\")\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.04700780572419774\n",
      "Micro Precision:\n",
      " 0.04715503741082304\n",
      "Micro Recall:\n",
      " 0.04700780572419774\n",
      "Micro F-Score:\n",
      " 0.0470813064628214\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=1, measure=\"jaccard\")\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05151777970511708\n",
      "Micro Precision:\n",
      " 0.051688130873651233\n",
      "Micro Recall:\n",
      " 0.05151777970511708\n",
      "Micro F-Score:\n",
      " 0.051602814698983576\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=5, measure=\"jaccard\")\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
