{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_representations.tf_idf import TfIdf\n",
    "from src.data_representations.vector import Vector\n",
    "from src.data_representations.structure import Structure\n",
    "from src.data_representations.data_representations import BOW\n",
    "from src.classifiers.knn import Knn\n",
    "from src.classifiers.knn_deprecated import Knn as Knn2\n",
    "from src.preprocessing.preprocessing import Preprocessor\n",
    "from src.evaluation.evaluation import Evaluator\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_train = \"./data/songs_train.txt\"\n",
    "dataset_train = Preprocessor(filepath=filepath_train, read_limit=10000)\n",
    "filepath_test = \"./data/songs_test.txt\"\n",
    "dataset_test = Preprocessor(filepath=filepath_test, read_limit=100)\n",
    "\n",
    "# Create numerical representations of labels for mapping\n",
    "artists = list(set(dataset_train.artists) | set(dataset_test.artists))\n",
    "label_to_num = {artist:i for i, artist in enumerate(artists)}\n",
    "num_to_label = {value:key for key, value in label_to_num.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = [BOW(tok) for tok in dataset_train.tokenized]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "classifier = Knn2(training_examples, training_labels)\n",
    "\n",
    "# Test predictions\n",
    "test_examples = [BOW(tok) for tok in dataset_test.tokenized]\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using different Tversky settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.1$, $\\beta=0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.03\n",
      "Micro Precision:\n",
      " 0.15789473684210525\n",
      "Micro Recall:\n",
      " 0.03\n",
      "Micro F-Score:\n",
      " 0.050420168067226885\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.1, beta=0.9)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.2$, $\\beta=0.8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.3125\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08620689655172414\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.2, beta=0.8)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.3$, $\\beta=0.7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.3125\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08620689655172414\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.3, beta=0.7)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=\\frac{1}{3}$, $\\beta=\\frac{2}{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.2777777777777778\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08474576271186442\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=1/3, beta=2/3)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.4$, $\\beta=0.6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.22727272727272727\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08196721311475409\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.4, beta=0.6)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.6$, $\\beta=0.4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.20833333333333334\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08064516129032258\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.6, beta=0.4)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=\\frac{2}{3}$, $\\beta=\\frac{1}{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.20833333333333334\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08064516129032258\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=2/3, beta=1/3)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.7$, $\\beta=0.3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.06\n",
      "Micro Precision:\n",
      " 0.2222222222222222\n",
      "Micro Recall:\n",
      " 0.06\n",
      "Micro F-Score:\n",
      " 0.09448818897637795\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.7, beta=0.3)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.8$, $\\beta=0.2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.06\n",
      "Micro Precision:\n",
      " 0.18181818181818182\n",
      "Micro Recall:\n",
      " 0.06\n",
      "Micro F-Score:\n",
      " 0.09022556390977443\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.8, beta=0.2)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0.1$, $\\beta=0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.03125\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.015151515151515152\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.9, beta=0.1)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=1$, $\\beta=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.03225806451612903\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.015267175572519083\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=1, beta=0)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\alpha=0$, $\\beta=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.02\n",
      "Micro Precision:\n",
      " 0.11764705882352941\n",
      "Micro Recall:\n",
      " 0.02\n",
      "Micro F-Score:\n",
      " 0.03418803418803419\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0, beta=1)\n",
    "# Run evaluation of algorithms performance\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "print(\"Micro F-Score:\\n\", evaluator.micro_fscore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_representations.tf_idf import TfIdf\n",
    "from src.data_representations.vector import Vector\n",
    "from src.data_representations.structure import Structure\n",
    "from src.classifiers.knn import Knn\n",
    "from src.classifiers.knn_deprecated import Knn as Knn2\n",
    "from src.preprocessing.preprocessing import Preprocessor\n",
    "from src.evaluation.evaluation import Evaluator\n",
    "from src.data_representations.data_representations import BOW\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(evaluator):\n",
    "    print(\"Accuracy:\\n\", evaluator.accuracy())\n",
    "    print(\"Micro Precision:\\n\", evaluator.micro_precision())\n",
    "    print(\"Micro Recall:\\n\", evaluator.micro_recall())\n",
    "    print(\"Micro F-Score:\\n\", evaluator.micro_fscore())\n",
    "\n",
    "def run_prediction(classifier, test_examples, test_labels, measure):\n",
    "    predictions = classifier.predict(test_examples, k=4, measure=measure)\n",
    "    evaluator = Evaluator(test_labels, predictions)\n",
    "    report(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "filepath_train = \"./data/songs_train.txt\"\n",
    "dataset_train = Preprocessor(filepath=filepath_train, read_limit=10000)\n",
    "filepath_test = \"./data/songs_test.txt\"\n",
    "dataset_test = Preprocessor(filepath=filepath_test, read_limit=100)\n",
    "\n",
    "# Create numerical representations of labels for mapping\n",
    "artists = list(set(dataset_train.artists) | set(dataset_test.artists))\n",
    "label_to_num = {artist:i for i, artist in enumerate(set(dataset_train.artists) | set(dataset_test.artists))}\n",
    "num_to_label = {value:key for key, value in label_to_num.items()}\n",
    "\n",
    "# how many process are gonna be run\n",
    "number_processes = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random chance\n",
    "This experiment is set up by picking a random artist from the list of artist N times (N being number of testing examples).\n",
    "\n",
    "Because it's random, we're taking the average of **10 tests**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.004\n",
      "Micro Precision:\n",
      " 0.02760854341736695\n",
      "Micro Recall:\n",
      " 0.004\n",
      "Micro F-Score:\n",
      " 0.006973639891062759\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "\n",
    "accuracy = []\n",
    "micro_precision = []\n",
    "micro_recall = []\n",
    "micro_fscore = []\n",
    "\n",
    "num_experiments = 10\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    random_labels = random.choices(range(len(artists)), k=len(test_labels))\n",
    "\n",
    "    evaluator = Evaluator(test_labels, random_labels)\n",
    "    accuracy.append(evaluator.accuracy())\n",
    "    micro_precision.append(evaluator.micro_precision())\n",
    "    micro_recall.append(evaluator.micro_recall())\n",
    "    micro_fscore.append(evaluator.micro_fscore())\n",
    "\n",
    "print(\"Accuracy:\\n\", sum(accuracy) / num_experiments)\n",
    "print(\"Micro Precision:\\n\", sum(micro_precision) / num_experiments)\n",
    "print(\"Micro Recall:\\n\", sum(micro_recall) / num_experiments)\n",
    "print(\"Micro F-Score:\\n\", sum(micro_fscore) / num_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = [BOW(tok) for tok in dataset_train.tokenized]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "classifier = Knn(training_examples, training_labels, multi_process=number_processes)\n",
    "\n",
    "# Test predictions\n",
    "test_examples = [BOW(tok) for tok in dataset_test.tokenized]\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.07\n",
      "Micro Precision:\n",
      " 0.3181818181818182\n",
      "Micro Recall:\n",
      " 0.07\n",
      "Micro F-Score:\n",
      " 0.11475409836065574\n"
     ]
    }
   ],
   "source": [
    "run_prediction(classifier, test_examples, test_labels, \"jaccard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sørensen-Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.07\n",
      "Micro Precision:\n",
      " 0.3181818181818182\n",
      "Micro Recall:\n",
      " 0.07\n",
      "Micro F-Score:\n",
      " 0.11475409836065574\n"
     ]
    }
   ],
   "source": [
    "run_prediction(classifier, test_examples, test_labels, \"dsc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Overlap index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.047619047619047616\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.01652892561983471\n"
     ]
    }
   ],
   "source": [
    "probably run_prediction(classifier, test_examples, test_labels, \"overlap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tversky "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.06\n",
      "Micro Precision:\n",
      " 0.2222222222222222\n",
      "Micro Recall:\n",
      " 0.06\n",
      "Micro F-Score:\n",
      " 0.09448818897637795\n"
     ]
    }
   ],
   "source": [
    "classifier = Knn2(training_examples, training_labels)\n",
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.7, beta=0.3)\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "report(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfIdf()\n",
    "train = tf_idf.fit_transform(dataset_train.tokenized)\n",
    "test = tf_idf.transform(dataset_test.tokenized)\n",
    "\n",
    "# Initiate Knn classifier\n",
    "training_examples = [Vector([ex]) for ex in train]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "\n",
    "classifier = Knn(training_examples, training_labels, number_processes)\n",
    "\n",
    "test_examples = [Vector([ex]) for ex in test]\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "del tf_idf, train, test, training_examples, training_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.07\n",
      "Micro Precision:\n",
      " 0.23333333333333334\n",
      "Micro Recall:\n",
      " 0.07\n",
      "Micro F-Score:\n",
      " 0.1076923076923077\n"
     ]
    }
   ],
   "source": [
    "run_prediction(classifier, test_examples, test_labels, \"cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.09090909090909091\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.018018018018018018\n"
     ]
    }
   ],
   "source": [
    "run_prediction(classifier, test_examples, test_labels, \"euclidean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print what artist was chosen and what is the real one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for pred, lab in zip(predictions, test_labels):\n",
    "#    print(num_to_label[pred], \" - \", num_to_label[lab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-idf + structural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfIdf()\n",
    "train = tf_idf.fit_transform(dataset_train.tokenized)\n",
    "test = tf_idf.transform(dataset_test.tokenized)\n",
    "\n",
    "train_struc =  Structure(dataset_train.tokenized)\n",
    "test_struc = Structure(dataset_test.tokenized)\n",
    "\n",
    "# Initiate Knn classifier\n",
    "training_examples = [Vector([ex, [n], [d]]) for ex, n, d in zip(train, train_struc.number_lines, train_struc.doc_length)]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "\n",
    "classifier = Knn(training_examples, training_labels, number_processes)\n",
    "\n",
    "test_examples = [Vector([ex, [n], [d]]) for ex, n, d in zip(test, test_struc.number_lines, test_struc.doc_length)]\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "del tf_idf, train, test, train_struc, test_struc \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.02\n",
      "Micro Precision:\n",
      " 0.08333333333333333\n",
      "Micro Recall:\n",
      " 0.02\n",
      "Micro F-Score:\n",
      " 0.03225806451612903\n"
     ]
    }
   ],
   "source": [
    "run_prediction(classifier, test_examples, test_labels, \"cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.05555555555555555\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.016949152542372885\n"
     ]
    }
   ],
   "source": [
    "run_prediction(classifier, test_examples, test_labels, \"euclidean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Bigger subsettraining and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_train = \"./data/songs_train.txt\"\n",
    "dataset_train = Preprocessor(filepath=filepath_train, read_limit=20000)\n",
    "filepath_test = \"./data/songs_test.txt\"\n",
    "dataset_test = Preprocessor(filepath=filepath_test, read_limit=100)\n",
    "\n",
    "# Create numerical representations of labels for mapping\n",
    "artists = list(set(dataset_train.artists) | set(dataset_test.artists))\n",
    "label_to_num = {artist:i for i, artist in enumerate(artists)}\n",
    "num_to_label = {value:key for key, value in label_to_num.items()}\n",
    "\n",
    "# how many process are gonna be run\n",
    "number_processes = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random chance\n",
    "This experiment is set up by picking a random artist from the list of artist N times (N being number of testing examples).\n",
    "\n",
    "Because it's random, we're taking the average of **10 tests**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.001\n",
      "Micro Precision:\n",
      " 0.006666666666666666\n",
      "Micro Recall:\n",
      " 0.001\n",
      "Micro F-Score:\n",
      " 0.0017391304347826088\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "\n",
    "accuracy = []\n",
    "micro_precision = []\n",
    "micro_recall = []\n",
    "micro_fscore = []\n",
    "\n",
    "num_experiments = 10\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    random_labels = random.choices(range(len(artists)), k=len(test_labels))\n",
    "\n",
    "    evaluator = Evaluator(test_labels, random_labels)\n",
    "    accuracy.append(evaluator.accuracy())\n",
    "    micro_precision.append(evaluator.micro_precision())\n",
    "    micro_recall.append(evaluator.micro_recall())\n",
    "    micro_fscore.append(evaluator.micro_fscore())\n",
    "\n",
    "print(\"Accuracy:\\n\", sum(accuracy) / num_experiments)\n",
    "print(\"Micro Precision:\\n\", sum(micro_precision) / num_experiments)\n",
    "print(\"Micro Recall:\\n\", sum(micro_recall) / num_experiments)\n",
    "print(\"Micro F-Score:\\n\", sum(micro_fscore) / num_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = [BOW(tok) for tok in dataset_train.tokenized]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "classifier = Knn(training_examples, training_labels, multi_process=number_processes)\n",
    "\n",
    "# Test predictions\n",
    "test_examples = [BOW(tok) for tok in dataset_test.tokenized]\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.09\n",
      "Micro Precision:\n",
      " 0.42857142857142855\n",
      "Micro Recall:\n",
      " 0.09\n",
      "Micro F-Score:\n",
      " 0.1487603305785124\n"
     ]
    }
   ],
   "source": [
    "run_prediction(classifier, test_examples, test_labels, \"jaccard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sørensen-Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.09\n",
      "Micro Precision:\n",
      " 0.42857142857142855\n",
      "Micro Recall:\n",
      " 0.09\n",
      "Micro F-Score:\n",
      " 0.1487603305785124\n"
     ]
    }
   ],
   "source": [
    "run_prediction(classifier, test_examples, test_labels, \"dsc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Overlap index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.05555555555555555\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.016949152542372885\n"
     ]
    }
   ],
   "source": [
    "run_prediction(classifier, test_examples, test_labels, \"overlap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tversky "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.05\n",
      "Micro Precision:\n",
      " 0.25\n",
      "Micro Recall:\n",
      " 0.05\n",
      "Micro F-Score:\n",
      " 0.08333333333333334\n"
     ]
    }
   ],
   "source": [
    "classifier = Knn2(training_examples, training_labels)\n",
    "predictions = classifier.predict(test_examples, k=4, measure=\"tversky\", alpha=0.7, beta=0.3)\n",
    "evaluator = Evaluator(test_labels, predictions)\n",
    "report(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = TfIdf()\n",
    "train = tf_idf.fit_transform(dataset_train.tokenized)\n",
    "test = tf_idf.transform(dataset_test.tokenized)\n",
    "print('done')\n",
    "\n",
    "# Initiate Knn classifier\n",
    "training_examples = [Vector([ex]) for ex in train]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "\n",
    "classifier = Knn(training_examples, training_labels, number_processes)\n",
    "\n",
    "test_examples = [Vector([ex]) for ex in test]\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "del tf_idf, train, test, training_examples, training_labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.08\n",
      "Micro Precision:\n",
      " 0.2857142857142857\n",
      "Micro Recall:\n",
      " 0.08\n",
      "Micro F-Score:\n",
      " 0.125\n"
     ]
    }
   ],
   "source": [
    "run_prediction(classifier, test_examples, test_labels, \"cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.04\n",
      "Micro Precision:\n",
      " 0.3333333333333333\n",
      "Micro Recall:\n",
      " 0.04\n",
      "Micro F-Score:\n",
      " 0.07142857142857142\n"
     ]
    }
   ],
   "source": [
    "run_prediction(classifier, test_examples, test_labels, \"euclidean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-idf + structural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfIdf()\n",
    "train = tf_idf.fit_transform(dataset_train.tokenized)\n",
    "test = tf_idf.transform(dataset_test.tokenized)\n",
    "\n",
    "train_struc =  Structure(dataset_train.tokenized)\n",
    "test_struc = Structure(dataset_test.tokenized)\n",
    "\n",
    "# Initiate Knn classifier\n",
    "training_examples = [Vector([ex, [n], [d]]) for ex, n, d in zip(train, train_struc.number_lines, train_struc.doc_length)]\n",
    "training_labels = [label_to_num[label] for label in dataset_train.artists]\n",
    "\n",
    "classifier = Knn(training_examples, training_labels, number_processes)\n",
    "\n",
    "test_examples = [Vector([ex, [n], [d]]) for ex, n, d in zip(test, test_struc.number_lines, test_struc.doc_length)]\n",
    "test_labels = [label_to_num[label] for label in dataset_test.artists]\n",
    "del tf_idf, train, test, train_struc, test_struc \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.05555555555555555\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.016949152542372885\n"
     ]
    }
   ],
   "source": [
    "run_prediction(classifier, test_examples, test_labels, \"cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.01\n",
      "Micro Precision:\n",
      " 0.07142857142857142\n",
      "Micro Recall:\n",
      " 0.01\n",
      "Micro F-Score:\n",
      " 0.01754385964912281\n"
     ]
    }
   ],
   "source": [
    "run_prediction(classifier, test_examples, test_labels, \"euclidean\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
